---
title: "A comparetive study on PCR, PLS, Envelope and BayesPLS models"
author: "Raju Rimal"
date: "4 September 2016"
output:
  beamer_presentation:
    colortheme: seahorse
    fonttheme: serif
    highlight: tango
    includes:
      in_header: header.tex
    keep_tex: yes
    theme: Madrid
    citation_package: natbib
    slide_level: 2
  ioslides_presentation:
    logo: LogoNMBU.png
    smaller: yes
    widescreen: yes
  slidy_presentation:
    highlight: tango
institute: | 
  | **Supervisors**
  | Solve Sæbø, Tryge Almøy
  | &
  | **Joint work with**
  | Inge Halland, UiO
csl: apa.csl
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
library(data.table)
library(ggplot2)
library(simrel)
```


## Overview

* Background
* Estimation methods under comparison
* Data Simulation
* Analysis, Results and Discussions

## Background

> - PLS **Population Model** [@helland1990partial] which further discussed by [@naes1993relevant; @helland2001some]
> - PLS, _heavely developed_ [@wold1985partial; @naes1993relevant; @de1993simpls], without addressing the population model [@cook2013envelopes]
> - Mostly popular among chemometrician
> - Was not very popular among statistician which has changed and is nowadays considered as an essential tool for multivariate analysis
> - Accounting the population model, new estimation methods have been purposed such as **Envelope** [@cook2010envelope; @cook2016algorithms] and **BayesPLS** [@helland2012near] which are _closely related_ to PLS
> - @cook2013envelopes said that PLS is fundamentally an envelope in the population model

## Background

> - This study attempts to make an _emperial comparison_ among PCR, PLS, Envelope and BayesPLS model on the basis of their **prediction ability**
> - Using `simrel` [@saebo2015simrel] R-package, data with diverse nature are simulated.
> - `simrel` allows to have control over latent structure (relevant component) of the data, fine analysis of strength and weakness of a models is possible

## Statistical Model

The common ground of all the methods is to best describe (fit) the multivariate linear model below,

\begin{equation}
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
\label{eq:model}
\end{equation}

where, 

|                         |   |                                               |
|-------------------------|---|-----------------------------------------------|
| $\boldsymbol{y}$        | : | Response                                      |
| $\boldsymbol{X}$        | : | Matrix of $p$ predictor variable              |
| $\boldsymbol{\beta}$    | : | Regression Coefficients                       |
| $\boldsymbol{\epsilon}$ | : | Error $\epsilon \sim \text{NID}(0, \sigma^2)$ |

Here, both $\boldsymbol{y}$ and $\boldsymbol{X}$ are considered to be centered.

## Statistical Model

All the models under this study consider a **subspace of predictor variables that is relevant for response**. They differ in the ways of finding the subspace and corresponding model estimates. The true estimates can also be written as,

$$
\begin{aligned}
\boldsymbol{\beta} &= \Sigma_{XX}^{-1}\sigma_{Xy} = \sum_{j=1}^p \frac{1}{\alpha_j}\boldsymbol{e}_j\boldsymbol{e}_j^t\sigma_{Xy}
= \sum_{j=1}^p\gamma_j\boldsymbol{e}_j\\
\end{aligned}
$$

where,

|                    |                                                     |
|--------------------|-----------------------------------------------------|
| $\gamma_j$         | : $\frac{\boldsymbol{e}_j^t\sigma_{Xy}}{\lambda_j}$ |
| $\boldsymbol{e}_j$ | : Eigenvector of $\Sigma_{xx}$                      |
| $\lambda_j$        | : Eigenvalue of $\Sigma_{xx}$                       |
| $\sigma_{Xy}$      | : Covariance between $y$ and $X$                    |

So, True regression estimates are the space spanned by the eigenvectors of population covariance matrix $\Sigma_{xx}$.

## Comparison of Methods

|                          PCR                          |                    PLS                    |
|-------------------------------------------------------|-------------------------------------------|
| * Regression of response on latent space of predictor | * Estimation through Iterative algorithm  |
| * No strict assumption                                | * No strict assumption                    |


. . .



|                        Envelope (MLE)                        |                               Bayes                                |
|--------------------------------------------------------------|--------------------------------------------------------------------|
| * Estimation using Maximum Likelihood                        | * Estimation through MCMC approach with rotation of relevant space |
| * Can not be used when predictor is larger than observations | * Heavy Computation when $p$ is large                              |


## Data Simulation
Models are analysed under diverse nature of data. Data are simulated using `simrel` package (R). In this study, I have included following four design;

```{r}
load("DesignDF.RData")
which.design <- c(1, 2, 27, 28)
knitr::kable(subDesign[which.design, -c("design", "q"), with = F])
```

|          |   |                                        |
|----------|---|----------------------------------------|
| `n`      | : | Number of observations                 |
| `p`      | : | Number of variables                    |
| `R2`     | : | Variation explained by the model       |
| `relpos` | : | Position of relevant components        |
| `gamma`  | : | Reduction factor of eigenvalue of $X$ |

For each of these design, 5000 test samples are simulated.

## Relevant Position and Eigenvalues

```{r}
load("simObj.Rdata")
getScreePlot <- function(DataList){
  egnMat <- lapply(DataList, function(x){
    data.table(x$lambda)[, n := .I]
  })
  egnMat <- rbindlist(egnMat, idcol = TRUE)
  setnames(egnMat, names(egnMat), c("Design", "Eigenvalue", "Variable"))
  egnMat[, Design := as.factor(Design)]
  
  relpos <- rbindlist(lapply(DataList, function(x){
    data.table(x$relpos)[, n := .I]
  }), idcol = TRUE)
  setnames(relpos, names(relpos), c("Design", "relpos", "Variable"))
  relpos[, Design := as.factor(Design)]
  relpos <- merge(relpos, egnMat, by = c('Design', 'Variable'))
  
  plt <- ggplot(egnMat, aes(Variable, Eigenvalue)) + 
    geom_line() + geom_point() + 
    facet_grid(.~Design, scales = 'free_x', labeller = label_both) +
    coord_cartesian(xlim = c(1, 15)) +
    geom_point(data = relpos, aes(x = relpos, y = 0), color = "red") +
    scale_x_continuous(breaks = seq(1, 15, 2))
  return(plt)
}
```

```{r, fig.height=3}
getScreePlot(subSimObj)
```

- When Relevant components are at the position of high eigenvalues, the situation is easier to model
- When Relevant components are at the position of low eigenvalues, for example 5, 10, then the most variation present in $X$ are not relevant for $Y$ and this will become a very difficult situation.

## Model assessment
Models are compared on the basis of their prediction ability by measuring _test_ and _training_ **Mean Square Error of Prediction (_MSEP_)**. Mean prediction error is calculated as,

$$
\begin{aligned}
\left(\text{Prediction Error}\right)_\text{training} &= \frac{1}{n}\sum_{i = 1}^n\left({\boldsymbol{y}_i - \hat{\boldsymbol{y}}_i}\right)^2 = \frac{1}{n}\sum_{i = 1}^n\left({\boldsymbol{y}_i - \left(\hat{\boldsymbol{\beta}}_0 + \hat{\boldsymbol{\beta}}\boldsymbol{X}_i\right)}\right)^2 \\
\left(\text{Prediction Error}\right)_\text{test} &= \frac{1}{n}\sum_{i = 1}^\text{ntest}\left({\boldsymbol{y}_{i\left(\text{test}\right)} - \hat{\boldsymbol{y}}_{i\left(\text{test}\right)}}\right)^2 \\ &= \frac{1}{n}\sum_{i = 1}^\text{ntest}\left({\boldsymbol{y}_{i\left(\text{test}\right)} - \left(\hat{\boldsymbol{\beta}}_0 + \hat{\boldsymbol{\beta}}\boldsymbol{X}_{i\left(\text{test}\right)}\right)}\right)^2
\end{aligned}
$$ 


## Analysis Results

```{r}
load("predErr.Rdata")
load("linearPredErr.Rdata")
linearErr <- melt(linearErr, 1:2, variable.name = "Type", 
                  value.name = "MSEP")[design %in% which.design & ncomp == 1]
predErr <- melt(predErr, 1:3, variable.name = "Type", 
                value.name = "MSEP")[design %in% which.design]
predErr$model <- c("Envelope", "PCR", "PLS", "Bayes")[match(predErr$model, unique(predErr$model))]
getPlot <- function(dgn, annot = TRUE) {
  predErr$design <- factor(predErr$design, labels = 1:4)
  dta <- predErr[design %in% as.character(dgn)]
  linearErr$design <- factor(linearErr$design, labels = 1:4)
  linearErr <- linearErr[design %in% as.character(dgn)]
  tbl <- gridExtra::tableGrob(
    as.data.frame(t(unlist(subDesign[which.design, -"design", with = F][dgn]))), 
    theme = gridExtra::ttheme_default(base_size = 9)
  )
  plt <- ggplot(dta, aes(ncomp, MSEP, color = model)) + 
    geom_line() + geom_point() + 
    facet_grid(Type ~ design, labeller = label_both) +
    coord_cartesian(ylim = c(0, 1.3)) +
    scale_color_brewer(name = "Models", palette = "Set1") +
    scale_x_continuous(breaks = 0:10) +
    theme(legend.position = "top") +
    geom_hline(data = linearErr, aes(yintercept = MSEP, color = "Least Square"))
  if (annot) plt <- plt + annotation_custom(tbl, xmin = 5.5, ymin = 0.95)
  return(plt)
}
```

```{r, fig.width=8, fig.align='center'}
getPlot(1)
```

## Analysis Results

```{r, fig.width=8, fig.align='center'}
getPlot(2)
```

## Analysis Results

```{r, fig.width=8, fig.align='center'}
getPlot(3)
```

## Analysis Results

```{r, fig.width=8, fig.align='center'}
getPlot(4)
```

## Analysis Results

```{r, fig.align='center'}
getPlot(1:4, annot = FALSE)
```


## Conclusion

> - New methods -- Envelope and Bayes, as they claim, are performing better than algorithmic approach of PLS
> - However, the performance of MLE approach of Envelope is not satisfactory when number of variable is large
> - In the case of Bayes PLS, the prediction error does not raises noticably (test prediction) after capturing enough information with few components
> - This suggests that it is able to find the direction of maximum variation after successive rotations of predictor subspace
> - The computation regarding BayesPLS is intensive which will not be fisible in case of wide dataset (very common in genomic data)
> - All the models are performing better than the least square solution


----

```{r, fig.align='center', fig.width=12}
knitr::include_graphics('ThankYou.jpg')
```


# References
<!-- \small -->
